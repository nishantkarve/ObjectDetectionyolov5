{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone YOLOv5 and \n",
    "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt # install dependencies\n",
    "%pip install -q roboflow\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the initial setup is complete, go to <a href = \"https://www.makesense.ai/\">MakeSense AI </a> and start the image labeling process. yolov5 requires annotated labels in a certain format.\n",
    "\n",
    "This can be done in AWS Sagemake Ground Truth as well. However, Amazon Sagemaker Ground Truth will generate an augmented manifest file, which will need to be converted to yolov5 format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd yolov5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install ffmpeg libsm6 libxext6  -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to yolov5/data and download the coco128.yaml file and make the following changes\n",
    "\n",
    "- Set the path to your trianing data\n",
    "- Set the path to your validation data\n",
    "- Set the number of classes. In our case we are predicting hard hats and hence number of classes is 1\n",
    "- Set the name of the label. In our case, it's only 1 and hence remove all other labels\n",
    "\n",
    "Once you make these changes save the file as custom_data.yaml and upload it to yolov5/data folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the train.py script to start training your model \n",
    "\n",
    "- img 416 indicates the size of the image\n",
    "- batch 1 indicates 1 file is sent at a time for training. You can modify this based on the resources you have.\n",
    "- epochs 30 indicates that we will train the model for 30 passes. You can increase this number for better accuracy\n",
    "- custom_data.yaml has paths for our train and test data as well as the label details\n",
    "- We start with yolov5 pretrained weights. This is an example of transfer learning\n",
    "\n",
    "The model will train for the number of set epochs and at the end of the training will provide the trained weights that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 416 --batch 1 --epochs 30 --data custom_data.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, you can go to yolov5/runs/detect/exp to check for the training results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "runs/train/exp5/weights/last.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py  --weights runs/train/exp5/weights/last.pt --img 640 --conf 0.25 --source ../Manufacturing\\ Safety\\ Employee\\ Video.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the detect.py script to detect hardhats in a video or an image\n",
    "\n",
    "- weights indicate our best trained weights after \"n\" epochs\n",
    "- img 640 will set the video frame size\n",
    "- conf 0.25 indicates show all detections with a confidence score of 25% and above\n",
    "- source is the source of the data we need to run the detection on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the detection on the frames is complete, upload the video to the S3 bucket and then download it to your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp runs/detect/exp/Manufacturing\\ Safety\\ Employee\\ Video.mp4 s3://ground-truth-labeling-test-v1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
